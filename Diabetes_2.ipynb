{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6082ae9c-9bec-4c51-8b63-19a5910b3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class DiabetesAnalysis:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        return self.data\n",
    "\n",
    "    def data_summary(self):\n",
    "        return self.data.describe()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Separate features and target\n",
    "        X = self.data.drop('Outcome', axis=1)\n",
    "        y = self.data['Outcome']\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Scale the features\n",
    "        self.X_train = self.scaler.fit_transform(X_train)\n",
    "        self.X_test = self.scaler.transform(X_test)\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "    \n",
    "\n",
    "    def hyperparameter_tuning(self):\n",
    "        # Define the parameter grid for RandomizedSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300, 400, 500],\n",
    "            'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "\n",
    "        # Initialize RandomizedSearchCV with RandomForestClassifier\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=RandomForestClassifier(random_state=42),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=15,  # Number of parameter combinations to try\n",
    "            cv=3,       # 5-fold cross-validation\n",
    "            n_jobs=-1,  # Use all available CPU cores\n",
    "            verbose=2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Fit to the training data\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Get the best parameters\n",
    "        best_params = random_search.best_params_\n",
    "        print(\"Best parameters found: \", best_params)\n",
    "\n",
    "        # Train the final model with the best parameters\n",
    "        self.model = RandomForestClassifier(**best_params, random_state=42)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Preprocess the data before training\n",
    "        self.preprocess_data()\n",
    "\n",
    "        # Perform hyperparameter tuning\n",
    "        self.hyperparameter_tuning()\n",
    "\n",
    "        # Predict on the test set\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, predictions))\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        # Scale the input data\n",
    "        input_data_scaled = self.scaler.transform([input_data])\n",
    "        prediction = self.model.predict(input_data_scaled)\n",
    "        prediction_proba = self.model.predict_proba(input_data_scaled)\n",
    "        print(f\"Prediction: {'Diabetic' if prediction[0] == 1 else 'Non-Diabetic'}\")\n",
    "        print(f\"Prediction Probability: {prediction_proba[0]}\")\n",
    "        return prediction[0]\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        # Plot feature importance if available\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            feature_importances = self.model.feature_importances_\n",
    "            features = self.data.drop('Outcome', axis=1).columns\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x=feature_importances, y=features)\n",
    "            plt.title('Feature Importance')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.show()\n",
    "\n",
    "    def plot_correlation_matrix(self):\n",
    "        # Plot the correlation matrix of the dataset\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(self.data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97d4e871-20e3-4756-9a6c-2efef730d826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Summary:\n",
      "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
      "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
      "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
      "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
      "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
      "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
      "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
      "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
      "\n",
      "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
      "count  768.000000                768.000000  768.000000  768.000000  \n",
      "mean    31.992578                  0.471876   33.240885    0.348958  \n",
      "std      7.884160                  0.331329   11.760232    0.476951  \n",
      "min      0.000000                  0.078000   21.000000    0.000000  \n",
      "25%     27.300000                  0.243750   24.000000    0.000000  \n",
      "50%     32.000000                  0.372500   29.000000    0.000000  \n",
      "75%     36.600000                  0.626250   41.000000    1.000000  \n",
      "max     67.100000                  2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Summary:\")\n",
    "print(analyzer.data_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "694eba6e-2715-48d6-aa2f-d2f500024b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model with hyperparameter tuning...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters found:  {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40}\n",
      "Model Accuracy: 0.7576\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81       151\n",
      "           1       0.65      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.73      0.74      0.73       231\n",
      "weighted avg       0.76      0.76      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining the model with hyperparameter tuning...\")\n",
    "analyzer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "776766a1-ce71-4257-88e0-c19db89d1d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pmls\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3523de2-8ceb-456a-a2da-c95d74dece56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE  # For data balancing\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77af815f-45f3-4d32-85c7-56613f30e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiabetesAnalysis:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.smote = SMOTE(random_state=42)\n",
    "        self.feature_engineer = False\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        return self.data\n",
    "\n",
    "    def data_summary(self):\n",
    "        return self.data.describe()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Separate features and target\n",
    "        X = self.data.drop('Outcome', axis=1)\n",
    "        y = self.data['Outcome']\n",
    "\n",
    "        # Feature Engineering: Adding Polynomial Features (Optional)\n",
    "        if self.feature_engineer:\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "            X = poly.fit_transform(X)\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Apply SMOTE to balance classes\n",
    "        X_train_balanced, y_train_balanced = self.smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Scale the features\n",
    "        self.X_train = self.scaler.fit_transform(X_train_balanced)\n",
    "        self.X_test = self.scaler.transform(X_test)\n",
    "        self.y_train = y_train_balanced\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def enable_feature_engineering(self):\n",
    "        self.feature_engineer = True\n",
    "\n",
    "    def train_model(self):\n",
    "        # Preprocess the data before training\n",
    "        self.preprocess_data()\n",
    "\n",
    "        # Train the RandomForest model\n",
    "        self.model = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(self.y_test, predictions)\n",
    "        print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, predictions))\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        # Scale the input data\n",
    "        input_data_scaled = self.scaler.transform([input_data])\n",
    "        prediction = self.model.predict(input_data_scaled)\n",
    "        prediction_proba = self.model.predict_proba(input_data_scaled)\n",
    "        print(f\"Prediction: {'Diabetic' if prediction[0] == 1 else 'Non-Diabetic'}\")\n",
    "        print(f\"Prediction Probability: {prediction_proba[0]}\")\n",
    "        return prediction[0]\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        # Plot feature importance if available\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            feature_importances = self.model.feature_importances_\n",
    "            features = self.data.drop('Outcome', axis=1).columns\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x=feature_importances, y=features)\n",
    "            plt.title('Feature Importance')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.show()\n",
    "\n",
    "    def plot_correlation_matrix(self):\n",
    "        # Plot the correlation matrix of the dataset\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(self.data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d45f1bc4-faae-4939-b982-2eac77752c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "analyzer = DiabetesAnalysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cd8632a-777a-42ba-a6a5-e27b6a6bee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enable feature engineering\n",
    "analyzer.enable_feature_engineering()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "928e0b4f-8e4b-4fcf-9978-a0f96f88175b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Summary:\n",
      "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
      "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
      "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
      "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
      "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
      "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
      "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
      "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
      "\n",
      "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
      "count  768.000000                768.000000  768.000000  768.000000  \n",
      "mean    31.992578                  0.471876   33.240885    0.348958  \n",
      "std      7.884160                  0.331329   11.760232    0.476951  \n",
      "min      0.000000                  0.078000   21.000000    0.000000  \n",
      "25%     27.300000                  0.243750   24.000000    0.000000  \n",
      "50%     32.000000                  0.372500   29.000000    0.000000  \n",
      "75%     36.600000                  0.626250   41.000000    1.000000  \n",
      "max     67.100000                  2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Load data from your specific location\n",
    "data = analyzer.load_data(r'C:\\Users\\PMLS\\Desktop\\diabetes.csv')\n",
    "\n",
    "# Display basic data info\n",
    "print(\"\\nData Summary:\")\n",
    "print(analyzer.data_summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6d989a1-283c-4074-be03-9e9909b2935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Model Accuracy: 0.7619\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81       151\n",
      "           1       0.63      0.76      0.69        80\n",
      "\n",
      "    accuracy                           0.76       231\n",
      "   macro avg       0.74      0.76      0.75       231\n",
      "weighted avg       0.78      0.76      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model with data balancing and feature engineering\n",
    "print(\"\\nTraining the model...\")\n",
    "analyzer.train_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81963a17-cc3a-4058-b50b-a24ba73573a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making prediction for sample patient...\n"
     ]
    }
   ],
   "source": [
    "# Make prediction with your sample input\n",
    "print(\"\\nMaking prediction for sample patient...\")\n",
    "sample_input = [6, 148, 72, 35, 0, 33.6, 0.627, 50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93724499-cb6e-4bde-b947-ab36a0a7c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input values:\n",
      "Pregnancies: 6\n",
      "Glucose: 148\n",
      "BloodPressure: 72\n",
      "SkinThickness: 35\n",
      "Insulin: 0\n",
      "BMI: 33.6\n",
      "DiabetesPedigree: 0.627\n",
      "Age: 50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8 features, but StandardScaler is expecting 36 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Plot feature importance\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating feature importance plot...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 60\u001b[0m, in \u001b[0;36mDiabetesAnalysis.predict\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Scale the input data\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     input_data_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(input_data_scaled)\n\u001b[0;32m     62\u001b[0m     prediction_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict_proba(input_data_scaled)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8 features, but StandardScaler is expecting 36 features as input."
     ]
    }
   ],
   "source": [
    "# Show what these values represent\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigree', 'Age']\n",
    "print(\"\\nInput values:\")\n",
    "for feature, value in zip(features, sample_input):\n",
    "    print(f\"{feature}: {value}\")\n",
    "\n",
    "# Make prediction\n",
    "analyzer.predict(sample_input)\n",
    "\n",
    "# Plot feature importance\n",
    "print(\"\\nGenerating feature importance plot...\")\n",
    "analyzer.plot_feature_importance()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Show correlation heatmap\n",
    "print(\"\\nGenerating correlation matrix...\")\n",
    "analyzer.plot_correlation_matrix()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25978bf-1d76-49c5-ac8a-4add37e668d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
